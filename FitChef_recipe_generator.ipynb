{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shadenAlsaif/FitChef_recipe_generator/blob/main/FitChef_recipe_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhueUX9QiGR7"
      },
      "source": [
        "# **FitChef** is an intelligent multimodal system for generating personalized food recipes. It uses a T5 model for text generation and a Diffusion model to create realistic food images, based on user inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUBt_xAbmKbw"
      },
      "source": [
        "# ✅ Step 1: Install required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "urSZKQzgG45j",
        "outputId": "7f344ac4-413f-4cee-c9e1-b8772e6ac14e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting huggingface-hub==0.26.2\n",
            "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting transformers==4.46.1\n",
            "  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers==0.20.1\n",
            "  Downloading tokenizers-0.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting diffusers==0.31.0\n",
            "  Downloading diffusers-0.31.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.26.2) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.26.2) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.26.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.26.2) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.26.2) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.26.2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.26.2) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.1) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.1) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.46.1) (0.5.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers==0.31.0) (8.7.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers==0.31.0) (11.2.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers==0.31.0) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.26.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.26.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.26.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub==0.26.2) (2025.4.26)\n",
            "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diffusers-0.31.0-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, diffusers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.30.2\n",
            "    Uninstalling huggingface-hub-0.30.2:\n",
            "      Successfully uninstalled huggingface-hub-0.30.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.33.1\n",
            "    Uninstalling diffusers-0.33.1:\n",
            "      Successfully uninstalled diffusers-0.33.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "Successfully installed diffusers-0.31.0 huggingface-hub-0.26.2 tokenizers-0.20.1 transformers-4.46.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade huggingface-hub==0.26.2 transformers==4.46.1 tokenizers==0.20.1 diffusers==0.31.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bGmuyb818-k0",
        "outputId": "f054c20a-492b-4bca-a18b-43a57875f400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m481.3/491.4 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets pandas scikit-learn --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg1vZ9S2SNIo",
        "outputId": "d9c138c5-569f-4472-a49c-80671be78ee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install evaluate --quiet\n",
        "!pip install --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "alSvqbHFftA5",
        "outputId": "d3e9568e-63be-4af9-a338-2db25e472da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=84164df35a8e4bae82bc0e6b4bc1e192bcc7314e8df3ebdfea888f9ed22c3214\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBQvy6JVfewG",
        "outputId": "ad76529a-b7ba-4db7-b068-2b6018d0f541"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMrEVtRbnZNi",
        "outputId": "e41005ca-1ed7-413c-af72-8d70f78419cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m135.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install gradio diffusers transformers accelerate scipy ftfy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldAzlyvA4FNj",
        "outputId": "702bf0ee-b7c2-48c5-b5fc-09667836e41d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.46.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.20.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bert_score\n",
            "Successfully installed bert_score-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!pip install bert_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnBJq3SNnsnj"
      },
      "source": [
        "# ✅ Step 2: Import necessary modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3XvysjNpLBN8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from transformers import get_scheduler\n",
        "from transformers import EarlyStoppingCallback\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from transformers import T5Tokenizer\n",
        "import re\n",
        "import gradio as gr\n",
        "import json\n",
        "import evaluate\n",
        "import random\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tr4fG9soZp4"
      },
      "source": [
        "# ✅ Step 3: Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff-wqkifRpLG",
        "outputId": "1fca034b-c169-4e86-a404-88b92b4fe7ee"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "mount failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         )\n\u001b[0;32m--> 279\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH6VCYG-ofN0"
      },
      "source": [
        "# ✅ Step 4: Load and inspect the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PKvq7tDL8-nc"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Advanced/RecipeNLG_dataset.csv\").head(700)\n",
        "data = data.sample(n=700, random_state=42).reset_index(drop=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "egvSJdEGZEm9"
      },
      "outputs": [],
      "source": [
        "# Remove unnecessary columns from the DataFrame to clean the dataset\n",
        "# 'Unnamed: 0' is often an index column from CSV export\n",
        "# 'link' and 'source' are external references not needed for generation\n",
        "# 'NER' (Named Entity Recognition) info isn't used in this context\n",
        "data.drop(columns=['Unnamed: 0', 'link', 'source', 'NER'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7vzNAKQyoEQO"
      },
      "outputs": [],
      "source": [
        "def infer_goal(ingredients):\n",
        "    # Combine all ingredients into one lowercase string for easier keyword search\n",
        "    ingredients_text = \" \".join(ingredients).lower()\n",
        "\n",
        "    # Check for high-protein ingredients, suggesting goal is muscle gain\n",
        "    if any(x in ingredients_text for x in ['chicken', 'beef', 'tofu', 'lentils', 'eggs']):\n",
        "        return \"gain muscle\"\n",
        "\n",
        "    # Check for low-calorie or light ingredients, suggesting goal is weight loss\n",
        "    elif any(x in ingredients_text for x in ['lettuce', 'zucchini', 'light', 'low-fat', 'low calorie']):\n",
        "        return \"lose weight\"\n",
        "\n",
        "    # If no specific ingredients found, assume goal is weight maintenance\n",
        "    else:\n",
        "        return \"maintain weight\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7yWX4Xy9YRUy"
      },
      "outputs": [],
      "source": [
        "# Infers dietary restrictions based on the presence or absence of certain ingredients:\n",
        "# - If both \"salt\" and \"soy sauce\" are missing, it's likely \"low sodium\".\n",
        "# - If all dairy products are missing, it's labeled as \"dairy-free\".\n",
        "# - If all animal products are missing, it's considered \"vegan\".\n",
        "# - If no gluten sources like flour or pasta are present, it's \"gluten-free\".\n",
        "# - If none of the above conditions apply, it returns \"none\" (no restriction).\n",
        "def infer_restriction(ingredients):\n",
        "    ingredients_text = \" \".join(ingredients).lower()\n",
        "    if \"salt\" not in ingredients_text and \"soy sauce\" not in ingredients_text:\n",
        "        return \"low sodium\"\n",
        "    elif all(x not in ingredients_text for x in ['milk', 'cheese', 'butter', 'cream']):\n",
        "        return \"dairy-free\"\n",
        "    elif all(x not in ingredients_text for x in ['meat', 'chicken', 'fish', 'eggs']):\n",
        "        return \"vegan\"\n",
        "    elif all(x not in ingredients_text for x in ['flour', 'bread', 'pasta']):\n",
        "        return \"gluten-free\"\n",
        "    else:\n",
        "        return \"none\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-UlAYN4CoJuJ"
      },
      "outputs": [],
      "source": [
        "# Apply goal and restriction inference functions to each row of the dataset:\n",
        "# - 'goal' column is filled by analyzing the ingredients to guess fitness goal (e.g., gain muscle, lose weight).\n",
        "# - 'restriction' column is filled based on dietary restrictions inferred from the ingredients.\n",
        "data['goal'] = data['ingredients'].apply(infer_goal)\n",
        "data['restriction'] = data['ingredients'].apply(infer_restriction)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCotjnDho7SZ"
      },
      "source": [
        "# ✅ Step 5: Define prompt/target creation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wA6NnYB5BVuV"
      },
      "outputs": [],
      "source": [
        "# This function creates a text prompt for the AI model based on each row of the dataset.\n",
        "# It incorporates the user's goal, dietary restriction, and a list of ingredients to guide the recipe generation.\n",
        "def create_prompt(row):\n",
        "    return f\"\"\"Create a recipe for {row['goal']} using only these ingredients: {row['ingredients']}.\n",
        "The recipe must be {row['restriction']}.\n",
        "List the title, ingredients, and step-by-step instructions.\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FU42qxEsBbSm"
      },
      "outputs": [],
      "source": [
        "# This function formats the recipe output (target) for training or evaluation.\n",
        "# It includes the recipe title and a numbered list of instructions.\n",
        "# It handles different data types: lists of steps or strings that may need parsing.\n",
        "def create_target(row):\n",
        "    directions = row['directions']\n",
        "\n",
        "    if isinstance(directions, list):\n",
        "        formatted_directions = \"\\n\".join([f\"{i+1}. {step}\" for i, step in enumerate(directions)])\n",
        "    elif isinstance(directions, str):\n",
        "        try:\n",
        "            import ast\n",
        "            dir_list = ast.literal_eval(directions)\n",
        "            formatted_directions = \"\\n\".join([f\"{i+1}. {step}\" for i, step in enumerate(dir_list)])\n",
        "        except:\n",
        "            formatted_directions = directions\n",
        "    else:\n",
        "        formatted_directions = str(directions)\n",
        "\n",
        "    return f\"Title: {row['title']}\\nInstructions:\\n{formatted_directions}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dKJNrbPPTRs-"
      },
      "outputs": [],
      "source": [
        "# Apply prompt/target creation\n",
        "data['cleaned_prompt'] = data.apply(create_prompt, axis=1)\n",
        "data['cleaned_recipe'] = data.apply(create_target, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TMrZjry8IdAj"
      },
      "outputs": [],
      "source": [
        "# This line sets the maximum column width display in pandas so that long text (like ingredients or instructions) is fully visible.\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "# Displays the first 5 rows of the DataFrame to inspect data format and content.\n",
        "data.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ_8Q2XcpSqC"
      },
      "source": [
        "# ✅ Step 6: Split into training and validation sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KyYKwGe9RC-6"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZbIzv4fpjYp"
      },
      "source": [
        "# ✅ Step 7: Initialize T5 tokenizer and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xDGP06tX8-w3"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer for the Flan-T5-base model from Google\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "# Tokenize input prompts for training and testing\n",
        "train_inputs = tokenizer(list(train_data['cleaned_prompt']), truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
        "test_inputs = tokenizer(list(test_data['cleaned_prompt']), truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
        "\n",
        "# Tokenize target recipes (expected outputs) for training and testing\n",
        "train_targets = tokenizer(list(train_data['cleaned_recipe']), truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
        "test_targets = tokenizer(list(test_data['cleaned_recipe']), truncation=True, padding=\"max_length\", max_length=256, return_tensors=\"pt\")\n",
        "\n",
        "# Set the \"labels\" field to the tokenized targets — this is crucial for T5 training\n",
        "train_inputs[\"labels\"] = train_targets[\"input_ids\"]\n",
        "test_inputs[\"labels\"] = test_targets[\"input_ids\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcXeIIK_rDwb"
      },
      "source": [
        "# ✅ Step 8:Create the training  and test dataset using Hugging Face's Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Qns9kYlU-AJo"
      },
      "outputs": [],
      "source": [
        "# The dataset is built from a dictionary containing input IDs, attention masks, and labels\n",
        "\n",
        "# Create the training dataset\n",
        "train_dataset = Dataset.from_dict({\n",
        "    'input_ids': train_inputs['input_ids'],         # Tokenized input sequences\n",
        "    'attention_mask': train_inputs['attention_mask'], # Attention masks to differentiate real tokens from padding\n",
        "    'labels': train_inputs['labels']                 # Target sequences (tokenized recipe outputs)\n",
        "})\n",
        "\n",
        "# Create the testing dataset\n",
        "test_dataset = Dataset.from_dict({\n",
        "    'input_ids': test_inputs['input_ids'],\n",
        "    'attention_mask': test_inputs['attention_mask'],\n",
        "    'labels': test_inputs['labels']\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tdwo_TvwT-x"
      },
      "source": [
        "# Step 9:**Fine-Tuning the T5 Model**\n",
        "We fine-tuned the google/flan-t5-base model to generate personalized recipes based on user input. Key adjustments included setting the number of epochs to 5, reducing the batch size to 4, increasing warm-up steps to 500, and configuring evaluation and checkpoint saving every 500 steps. We also enabled automatic loading of the best model and used TensorBoard for tracking training progress.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "m9TVy3rj8-0P"
      },
      "outputs": [],
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",                  # Directory to save model checkpoints and logs\n",
        "    num_train_epochs=5,                      # Reduced number of epochs to avoid overfitting\n",
        "    per_device_train_batch_size=4,           # Increased batch size for faster training\n",
        "    per_device_eval_batch_size=4,            # Batch size during evaluation\n",
        "    warmup_steps=500,                        # Gradually increase learning rate at the start\n",
        "    weight_decay=0.01,                       # Regularization to prevent overfitting\n",
        "    logging_dir=\"./logs\",                    # Directory to store training logs\n",
        "    logging_steps=50,                        # Log training info every 50 steps\n",
        "    evaluation_strategy=\"steps\",             # Evaluate every X steps\n",
        "    eval_steps=500,                          # Run evaluation every 500 steps\n",
        "    save_strategy=\"steps\",                   # Save model checkpoint every X steps\n",
        "    save_steps=500,                          # Save model checkpoint every 500 steps\n",
        "    save_total_limit=3,                      # Keep only the 3 most recent checkpoints\n",
        "    load_best_model_at_end=True,             # Always load the best model at the end of training\n",
        "    lr_scheduler_type=\"linear\",              # Linear learning rate decay (good for fine-tuning)\n",
        "    report_to=\"tensorboard\",                 # Enable TensorBoard for better visualization of logs\n",
        "    disable_tqdm=False                       # Enable progress bars for better monitoring\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E9p2jlTsyLC"
      },
      "source": [
        "# ✅ Step 10: Initialize the Trainer and start fine-tuning the T5 model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gCBZw0ShPiq5"
      },
      "outputs": [],
      "source": [
        "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                      # The T5 model for text generation\n",
        "    args=training_args,               # Training configuration\n",
        "    train_dataset=train_dataset,      # Dataset for training\n",
        "    eval_dataset=test_dataset,        # Dataset for evaluation during training\n",
        "    tokenizer=tokenizer,              # Tokenizer to preprocess inputs and decode outputs\n",
        "    callbacks=[early_stopping_callback]  # Add EarlyStoppingCallback\n",
        ")\n",
        "\n",
        "# Start training the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hIn1WXEuDpya"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "save_path = \"/content/fine_tuned_best_recipe_model\"\n",
        "trainer.model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print(\"Final model saved to:\", save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxVnb9MEtsLL"
      },
      "source": [
        "# ✅ Step 11 : Evaluation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DXui7Z40ogRG"
      },
      "outputs": [],
      "source": [
        "# Load the ROUGE evaluation metric\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "# Use the full test dataset for evaluation\n",
        "full_test = test_dataset\n",
        "\n",
        "# Set the device (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Generate predictions and collect references\n",
        "preds = []\n",
        "refs = []\n",
        "\n",
        "for example in full_test:\n",
        "    # Prepare input tensors and move to the device\n",
        "    input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device)\n",
        "    attention_mask = torch.ones_like(input_ids).to(device)  # Use actual attention_mask if available\n",
        "\n",
        "    # Generate output from the model\n",
        "    output_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "    # Decode generated prediction\n",
        "    pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    preds.append(pred)\n",
        "\n",
        "    # Decode reference label\n",
        "    ref = tokenizer.decode(example[\"labels\"], skip_special_tokens=True)\n",
        "    refs.append(ref)\n",
        "\n",
        "# Compute ROUGE scores\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "results = rouge.compute(predictions=preds, references=refs)\n",
        "\n",
        "# Print the evaluation results\n",
        "for k, v in results.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J1HHex4A7pvD"
      },
      "outputs": [],
      "source": [
        "# Define the paths for the training and validation JSONL files\n",
        "train_jsonl_path = \"/content/recipes_train.jsonl\"\n",
        "val_jsonl_path = \"/content/recipes_validation.jsonl\"\n",
        "\n",
        "# Function to vary the style of the recipe format\n",
        "def vary_recipe_style(text):\n",
        "    # List of different styles to modify the recipe text\n",
        "    styles = [\n",
        "        lambda x: x,  # Default style, no change\n",
        "        lambda x: x.replace(\"Title:\", \"Recipe Name:\").replace(\"Instructions:\", \"Steps:\"),  # Modify title and instructions labels\n",
        "        lambda x: \"Here's your delicious recipe:\\n\" + x,  # Add a phrase before the recipe\n",
        "        lambda x: x.replace(\"\\\\n\", \"\\n• \"),  # Replace newline character with bullet points\n",
        "    ]\n",
        "    # Return a randomly chosen style applied to the recipe text\n",
        "    return random.choice(styles)(text)\n",
        "\n",
        "# Function to generate multiple prompts for a given row of data\n",
        "def create_multiple_prompts(row):\n",
        "    # Base prompt asking to create a recipe with the given ingredients\n",
        "    base_prompt = f\"Create a recipe using: {row['ingredients']}\"\n",
        "\n",
        "    # List of varied prompts asking the user to generate a recipe with those ingredients\n",
        "    prompts = [\n",
        "        f\"Generate a recipe using these ingredients: {row['ingredients']}\",\n",
        "        f\"I have {row['ingredients']}. What can I cook?\",\n",
        "        f\"Make a dish with: {row['ingredients']}\"\n",
        "    ]\n",
        "\n",
        "    # Construct the target recipe format with title and instructions\n",
        "    target_recipe = f\"Title: {row['title']}\\nInstructions: {row['directions']}\"\n",
        "\n",
        "    # List to store the generated examples (prompts and target recipe pairs)\n",
        "    examples = []\n",
        "    for prompt in prompts:\n",
        "        # Create examples for training data\n",
        "        examples.append({\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "                {\"role\": \"assistant\", \"content\": vary_recipe_style(target_recipe)}  # Apply the style variation to the recipe\n",
        "            ]\n",
        "        })\n",
        "    return examples\n",
        "\n",
        "# Write the training data to the JSONL file\n",
        "with open(train_jsonl_path, 'w', encoding='utf-8') as f:\n",
        "    for _, row in train_data.iterrows():\n",
        "        # For each row in the training data, create multiple prompts and write them to the file\n",
        "        for example in create_multiple_prompts(row):\n",
        "            json.dump(example, f, ensure_ascii=False)  # Write each example as a JSON object\n",
        "            f.write('\\n')  # Ensure each example is written on a new line\n",
        "\n",
        "# Write the validation data to the JSONL file\n",
        "with open(val_jsonl_path, 'w', encoding='utf-8') as f:\n",
        "    for _, row in test_data.iterrows():\n",
        "        # For each row in the validation data, create one prompt and write it to the file\n",
        "        json.dump(create_multiple_prompts(row)[0], f, ensure_ascii=False)  # Write only the first example for validation\n",
        "        f.write('\\n')  # Ensure each example is written on a new line\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hjClbKsj3MMn"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned T5 model and tokenizer from the specified directory\n",
        "model_path = \"/content/fine_tuned_best_recipe_model\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load validation data containing prompts and reference outputs\n",
        "validation_path = \"/content/recipes_validation.jsonl\"\n",
        "prompts = []\n",
        "references = []\n",
        "\n",
        "# Parse the JSONL validation file\n",
        "with open(validation_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        user_msg = None\n",
        "        assistant_msg = None\n",
        "        # Extract user and assistant messages from the conversation\n",
        "        for msg in data[\"messages\"]:\n",
        "            if msg[\"role\"] == \"user\":\n",
        "                user_msg = msg[\"content\"]\n",
        "            elif msg[\"role\"] == \"assistant\":\n",
        "                assistant_msg = msg[\"content\"]\n",
        "        if user_msg and assistant_msg:\n",
        "            prompts.append(user_msg)\n",
        "            references.append(assistant_msg)\n",
        "\n",
        "# Generate predictions from the model for each prompt\n",
        "predictions = []\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "    # Tokenize the prompt and move it to the same device as the model\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
        "\n",
        "    # Generate output from the model with sampling parameters\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=600,       # Limit the output length\n",
        "        do_sample=True,           # Enable sampling\n",
        "        temperature=0.9,          # Control randomness\n",
        "        top_k=50,                 # Consider only top-k likely tokens\n",
        "        top_p=0.95,               # Nucleus sampling\n",
        "        num_beams=1               # No beam search, pure sampling\n",
        "    )\n",
        "\n",
        "    # Decode the generated token IDs to text\n",
        "    generated = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    predictions.append(generated)\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"[{i+1}/{len(prompts)}] ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PAxB8oyr3kAF"
      },
      "outputs": [],
      "source": [
        "# Load the BERTScore evaluation metric from the 'evaluate' library\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "# Compute BERTScore for predictions and references with English language setting\n",
        "score = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
        "\n",
        "# Print out the BERTScore metrics (Precision, Recall, and F1)\n",
        "print(\"BERTScore - Precision:\", sum(score[\"precision\"]) / len(score[\"precision\"]))\n",
        "print(\"BERTScore - Recall:\", sum(score[\"recall\"]) / len(score[\"recall\"]))\n",
        "print(\"BERTScore - F1:\", sum(score[\"f1\"]) / len(score[\"f1\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFfwDSC_0Hvx"
      },
      "source": [
        "# ✅ Step 12 : Format the output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zyhzWy-yDfWo"
      },
      "outputs": [],
      "source": [
        "def clean_output(text):\n",
        "\n",
        "    # Define the sections to search for in the text\n",
        "    sections = [\"title\", \"ingredients\", \"instructions\", \"preparation\", \"cooking\", \"nutritional\", \"info\"]\n",
        "\n",
        "    # Convert the text to lowercase for uniformity\n",
        "    text = text.lower()\n",
        "\n",
        "    # Initialize the result dictionary, current section and lines to store text\n",
        "    result = {}\n",
        "    current_section = None\n",
        "    lines = []\n",
        "\n",
        "    # Split the text into words and process each one\n",
        "    for word in text.split():\n",
        "        if word in sections:  # Check if the word matches a section\n",
        "            if current_section and lines:  # If a previous section exists, save the lines\n",
        "                result[current_section] = \" \".join(lines).strip()\n",
        "                lines = []  # Reset lines for the next section\n",
        "            current_section = word  # Update the current section\n",
        "        else:\n",
        "            lines.append(word)  # Add the word to the current section's lines\n",
        "\n",
        "    # After the loop, ensure the last section's text is added to the result\n",
        "    if current_section and lines:\n",
        "        result[current_section] = \" \".join(lines).strip()\n",
        "\n",
        "    # Format the results to include section names and their corresponding text\n",
        "    formatted = []\n",
        "    for section in result:\n",
        "        formatted.append(f\"{section.capitalize()}:\\n{result[section]}\\n\")\n",
        "\n",
        "    return \"\\n\".join(formatted)  # Join the sections with newlines and return the result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FUqo2YjPgb1E"
      },
      "outputs": [],
      "source": [
        "def get_user_input():\n",
        "    print(\"🍽️ Let's create your personalized recipe!\")\n",
        "\n",
        "    ingredients = input(\"🧾 What ingredients do you want in your recipe? (comma separated): \")\n",
        "    goal = input(\"🎯 What's your goal? (e.g., weight loss, muscle gain, low-carb, etc.): \")\n",
        "    restrictions = input(\"🚫 Any dietary restrictions? (e.g., vegan, gluten-free): \")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a precise recipe generator.\n",
        "\n",
        "Your job is to create a recipe using ONLY these ingredients: {ingredients}.\n",
        "Do NOT add any other ingredients.\n",
        "\n",
        "The recipe must help achieve the following goal: {goal},\n",
        "and it must follow these dietary restrictions: {restrictions}.\n",
        "\n",
        "Format:\n",
        "\n",
        "Title:\n",
        "Ingredients:\n",
        "Instructions:\n",
        "Preparation Time:\n",
        "Cooking Time:\n",
        "Nutritional Info:\n",
        "\"\"\"\n",
        "\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc5BRdimr5s_"
      },
      "outputs": [],
      "source": [
        "def generate_clean_recipe(model, tokenizer):\n",
        "    # Step 1: Get user input for the recipe prompt\n",
        "    prompt = get_user_input()  # This function gets the recipe ingredients or instructions from the user\n",
        "\n",
        "    # Step 2: Generate the raw recipe text using the prompt and the model\n",
        "    raw_text = generate_recipe(prompt, model, tokenizer)  # Uses the generate_recipe function to create text from the model\n",
        "\n",
        "    # Step 3: Clean the generated recipe text for better formatting or readability\n",
        "    cleaned = clean_output(raw_text)  # This function cleans the raw text (e.g., removes unwanted characters or formatting issues)\n",
        "\n",
        "    # Step 4: Display the cleaned, final recipe to the user\n",
        "    print(\"\\nYour Personalized Recipe:\\n\")  # Print a message indicating the recipe is ready\n",
        "    print(cleaned)  # Output the cleaned recipe to the console\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qZumkE4TtJ9l"
      },
      "outputs": [],
      "source": [
        "def extract_visual_prompt(recipe_text):\n",
        "    # Split the recipe into lines\n",
        "    lines = recipe_text.split(\"\\n\")\n",
        "    title = \"\"\n",
        "    ingredients = []\n",
        "\n",
        "    # Loop through each line to extract title and ingredients\n",
        "    for line in lines:\n",
        "        if line.lower().startswith(\"title:\"):\n",
        "            title = line[6:].strip()  # Extract the title text after \"title:\"\n",
        "        if line.startswith(\"•\") or line.strip().startswith(\"-\"):\n",
        "            ingredients.append(line.strip())  # Collect bullet-point or dash-started ingredients\n",
        "\n",
        "    # Use the first 3 ingredients as the main ones\n",
        "    main_ingredients = \", \".join(ingredients[:3])\n",
        "\n",
        "    # Create a visual prompt string to be used for image generation\n",
        "    return f\"{title}, made with {main_ingredients}, beautifully plated, food photography\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PHFmRc9BHRQg"
      },
      "outputs": [],
      "source": [
        "# Load the Stable Diffusion model from the Hugging Face model hub\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\").to(device)\n",
        "\n",
        "# Check if a GPU (CUDA) is available and set the device accordingly\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Move the image generation pipeline to the appropriate device (CPU or GPU)\n",
        "pipe = pipe.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FJV4CkNcTBlK"
      },
      "outputs": [],
      "source": [
        "def generate_recipe(prompt, model, tokenizer, max_new_tokens=1200):\n",
        "    # Tokenize the input prompt and prepare it for the model\n",
        "    inputs = tokenizer(prompt.strip(), return_tensors=\"pt\", truncation=True, max_length=1000)\n",
        "    # Move all inputs to the device (GPU or CPU)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    model.to(device)\n",
        "\n",
        "    # Generate the recipe with modified settings\n",
        "    output = model.generate(\n",
        "        **inputs, # Provide the tokenized input to the model\n",
        "        max_new_tokens=max_new_tokens, # Limit the maximum number of new tokens generated\n",
        "        do_sample=True,  # Enable sampling for more diverse outputs\n",
        "        repetition_penalty=1.2, # Penalize repeated words to avoid repetition in the output\n",
        "        temperature=1.0,  # Adjust temperature for more variation\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        num_beams=5  # Increase number of beams for better quality\n",
        "    )\n",
        "\n",
        "    # Decode the generated output to human-readable text\n",
        "    raw_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return raw_text  # Return the generated recipe text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1_b4JH3BgmHl"
      },
      "outputs": [],
      "source": [
        "# Get user input prompt (e.g., list of ingredients or a meal idea)\n",
        "user_prompt = get_user_input()\n",
        "\n",
        "# Generate a recipe using the fine-tuned T5 model\n",
        "generated_recipe = generate_recipe(user_prompt, model, tokenizer, max_new_tokens=600)\n",
        "\n",
        "# Print the raw generated recipe text\n",
        "print(generated_recipe)\n",
        "\n",
        "# Extract a visual prompt from the generated recipe (title + key ingredients)\n",
        "visual_prompt = extract_visual_prompt(generated_recipe)\n",
        "\n",
        "# Use the image generation pipeline to create a food image from the visual prompt\n",
        "image = pipe(visual_prompt).images[0]\n",
        "\n",
        "# Display the generated image\n",
        "image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-A8uFmo3zmo-"
      },
      "outputs": [],
      "source": [
        "def evaluate_image_recipe_alignment(recipe_text, image):\n",
        "    # Load the pre-trained CLIP model and processor from Hugging Face.\n",
        "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")  # The CLIP model is used to process text and image together.\n",
        "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")  # Processor for preprocessing text and image inputs.\n",
        "\n",
        "    # Preprocess the input text (recipe) and image for the model.\n",
        "    inputs = processor(text=[recipe_text], images=[image], return_tensors=\"pt\", padding=True)\n",
        "    # The processor tokenizes the text and prepares the image for the model. The inputs are returned as tensors for PyTorch.\n",
        "\n",
        "    # Forward pass through the model to get logits (raw predictions).\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image  # Get the image-to-text similarity logits from the model's output.\n",
        "\n",
        "    # Calculate the alignment score by applying softmax to the logits and selecting the highest value.\n",
        "    score = logits_per_image.softmax(dim=1).max().item()  # Softmax normalizes the logits and we take the maximum value as the alignment score.\n",
        "\n",
        "    return score  # Return the alignment score, representing how well the recipe text aligns with the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nQ9puiWT8QmM"
      },
      "outputs": [],
      "source": [
        "# Evaluate alignment (using CLIP)\n",
        "clip_score = evaluate_image_recipe_alignment(generated_recipe, image)\n",
        "print(f\"Recipe-Image Alignment Score: {clip_score:.2f}/1.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFPfA4fG__Nk"
      },
      "source": [
        "# ✅ Step 13 : Interface\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PQtsMjwk0ZXa"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model and tokenizer for recipe generation\n",
        "model = T5ForConditionalGeneration.from_pretrained('/content/fine_tuned_best_recipe_model')  # Load model\n",
        "tokenizer = T5Tokenizer.from_pretrained('/content/fine_tuned_best_recipe_model')  # Load corresponding tokenizer\n",
        "\n",
        "# Define device (GPU or CPU) for model execution\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Automatically select GPU if available\n",
        "model = model.to(device)  # Move model to the selected device\n",
        "\n",
        "def generate_recipe(ingredients, goal, restrictions):\n",
        "    prompt = f\"\"\"\n",
        "You are a precise recipe generator.\n",
        "\n",
        "Your job is to create a recipe using ONLY these ingredients: {ingredients}.\n",
        "Do NOT add any other ingredients.\n",
        "\n",
        "The recipe must help achieve the following goal: {goal},\n",
        "and it must follow these dietary restrictions: {restrictions}.\n",
        "\n",
        "Format:\n",
        "\n",
        "Title:\n",
        "Ingredients:\n",
        "Instructions:\n",
        "Preparation Time:\n",
        "Cooking Time:\n",
        "Nutritional Info:\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt.strip(), return_tensors=\"pt\", truncation=True, max_length=1002).to(device)\n",
        "\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=600,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        num_beams=1\n",
        "    )\n",
        "\n",
        "\n",
        "    # Decode the output text from the model's response\n",
        "    text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Generate an image corresponding to the recipe text\n",
        "    image = pipe(text).images[0]  # Assuming 'pipe' is already initialized for image generation\n",
        "\n",
        "    return text, image  # Return both the generated recipe and the image\n",
        "\n",
        "# Launch a Gradio interface to interact with the model and generate recipes\n",
        "gr.Interface(\n",
        "    fn=generate_recipe,  # Function to generate recipes\n",
        "    inputs=[  # Input fields for the user\n",
        "        gr.Textbox(label=\"🧾 Ingredients (comma separated)\", placeholder=\"e.g. pasta, tomatoes, garlic\"),\n",
        "        gr.Textbox(label=\"🎯 Goal\", placeholder=\"e.g. low-carb, weight loss\"),\n",
        "        gr.Textbox(label=\"🚫 Dietary Restrictions\", placeholder=\"e.g. vegan, gluten-free\")\n",
        "    ],\n",
        "    outputs=[  # Output fields for displaying the generated recipe and image\n",
        "        gr.Textbox(label=\"📜 Generated Recipe\"),\n",
        "        gr.Image(label=\"🖼️ Generated Dish Image\")\n",
        "    ],\n",
        "    title=\"🍽️ Recipe Generator\",  # Title of the app\n",
        "    description=\"\\nEnter your ingredients, goal, and dietary needs — get a full recipe with a generated image!\",  # Description of the app\n",
        "    theme=\"compact\",  # Adjust the interface theme\n",
        "    css=\"#interface {background-color: #FFFF00;}\"\n",
        "      ).launch(share=True)  # Launch the app with the option to share the link"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}